# __init__.py
from .core import NNGDB
from .cli import NNGDBREPL
from . import utils
from . import advanced

__version__ = "0.1.0"

__all__ = ['NNGDB', 'NNGDBREPL', 'utils', 'advanced']

# helpers.py
import glob

# Use glob to find all .py files in current directory and subdirectories
py_files = glob.glob('**/*.py', recursive=True)

# Open output.txt file in write mode
with open('output.txt', 'w') as f:
    # Write each file path to output.txt
    for file in py_files:
        f.write('# ' + file + '\n')
        with open(file, 'r') as py_file:
            f.write(py_file.read() + '\n\n')

# main.py
import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from core import NNGDB
from cli import NNGDBREPL

def main():
    parser = argparse.ArgumentParser(description="Neural Network GDB (NNGDB)")
    parser.add_argument("--model", type=str, default="gpt2", help="Model name or path")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", help="Device to run the model on")
    args = parser.parse_args()

    print(f"Loading model: {args.model}")
    model = AutoModelForCausalLM.from_pretrained(args.model).to(args.device)
    tokenizer = AutoTokenizer.from_pretrained(args.model)

    print(f"Initializing NNGDB")
    debugger = NNGDB(model, args.model, args.device)
    debugger.set_context('tokenizer', tokenizer)
    debugger.set_context('device', args.device)

    print("Starting NNGDB REPL")
    repl = NNGDBREPL(debugger)
    repl.run()

if __name__ == "__main__":
    main()

# breakpoints/__init__.py
from .breakpoint_manager import BreakpointManager
from .conditional_breakpoint import ConditionalBreakpoint

__all__ = ['BreakpointManager', 'ConditionalBreakpoint']

# breakpoints/breakpoint_manager.py
from typing import Dict, List, Optional
from .conditional_breakpoint import ConditionalBreakpoint
from core.model_wrapper import ModelWrapper

class BreakpointManager:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model
        self.breakpoints: Dict[str, List[ConditionalBreakpoint]] = {}

    def set_breakpoint(self, layer_name: str, condition: str = None):
        if layer_name not in self.breakpoints:
            self.breakpoints[layer_name] = []
        self.breakpoints[layer_name].append(condition)
        self.wrapped_model.set_breakpoint(layer_name, condition)
        return f"Breakpoint set at {layer_name}" + (f" with condition: {condition}" if condition else "")
    
    def remove_breakpoint(self, layer_name: str, index: int = -1):
        if layer_name in self.breakpoints:
            if 0 <= index < len(self.breakpoints[layer_name]):
                removed = self.breakpoints[layer_name].pop(index)
                if not self.breakpoints[layer_name]:
                    del self.breakpoints[layer_name]
                # Remove the breakpoint from the model wrapper
                self.wrapped_model.remove_breakpoint(layer_name)
                return f"Breakpoint removed at {layer_name}, index {index}"
            elif index == -1 and self.breakpoints[layer_name]:
                del self.breakpoints[layer_name]
                # Remove all breakpoints for this layer from the model wrapper
                self.wrapped_model.remove_breakpoint(layer_name)
                return f"All breakpoints removed at {layer_name}"
        return f"No breakpoint found at {layer_name}" + (f", index {index}" if index != -1 else "")

    def list_breakpoints(self) -> str:
        if not self.breakpoints:
            return "No breakpoints set"
        
        breakpoint_list = []
        for layer, bps in self.breakpoints.items():
            for idx, bp in enumerate(bps):
                breakpoint_list.append(f"{layer}[{idx}]: {bp}")
        
        return "\n".join(breakpoint_list)

    def clear_all_breakpoints(self):
        self.breakpoints.clear()
        self.wrapped_model.clear_all_breakpoints()
        return "All breakpoints cleared"

    def hit_breakpoint(self, layer_name: str, output):
        if layer_name in self.breakpoints:
            for bp in self.breakpoints[layer_name]:
                if bp.should_break(output):
                    print(f"Breakpoint hit at {layer_name}")
                    bp.execute_action(self.wrapped_model, output)
                    return True
        return False

# breakpoints/conditional_breakpoint.py
from typing import Optional
import torch

class ConditionalBreakpoint:
    def __init__(self, condition: Optional[str] = None, action: Optional[str] = None):
        self.condition = condition
        self.action = action

    def should_break(self, output) -> bool:
        if self.condition is None:
            return True
        try:
            return eval(self.condition, {'output': output, 'torch': torch})
        except Exception as e:
            print(f"Error evaluating breakpoint condition: {e}")
            return False

    def execute_action(self, model_wrapper, output):
        if self.action is not None:
            try:
                exec(self.action, {'self': model_wrapper, 'output': output, 'torch': torch})
            except Exception as e:
                print(f"Error executing breakpoint action: {e}")

    def __str__(self):
        return f"Condition: {self.condition or 'None'}, Action: {self.action or 'None'}"

# core/__init__.py
from .debugger import NNGDB
from .model_wrapper import ModelWrapper
from .execution_engine import ExecutionEngine
from .undo_manager import UndoManager

__all__ = ['NNGDB', 'ModelWrapper', 'ExecutionEngine', 'UndoManager']

# core/execution_engine.py
import torch
from .model_wrapper import ModelWrapper

class ExecutionEngine:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model
        self.current_layer_index = 0
        self.last_input = None

    def run(self, input_data: torch.Tensor):
        self.wrapped_model.execution_paused = False
        self.current_layer_index = 0
        self.last_input = input_data
        return self.wrapped_model(input_data)

    def continue_execution(self):
        if self.last_input is None:
            return "Error: No previous execution. Run the model first."
        self.wrapped_model.execution_paused = False
        print("Continuing execution...")
        return self.wrapped_model(self.last_input)

    def step(self, num_steps: int = 1):
        if not self.wrapped_model.layer_order:
            return "Error: Model layers not initialized. Run the model first."
        
        if self.current_layer_index is None:
            return "Error: No previous execution. Run the model first."
        
        target_index = min(self.current_layer_index + num_steps, len(self.wrapped_model.layer_order) - 1)
        target_layer = self.wrapped_model.layer_order[target_index]
        self.current_layer_index = target_index
        print(f"Stepping {num_steps} layer(s) to {target_layer}")
        return self.wrapped_model.step_to_layer(target_layer)

    def run_backward(self, loss: torch.Tensor):
        loss.backward()
        print("Backward pass completed. Gradients computed.")

    def reset(self):
        self.wrapped_model.reset_to_initial_state()
        self.wrapped_model.current_step = 0
        self.wrapped_model.current_layer = ""
        self.wrapped_model.execution_paused = False
        self.wrapped_model.step_mode = False
        print("Execution engine reset.")

# core/undo_manager.py
class UndoManager:
    def __init__(self, max_history=10):
        self.history = []
        self.max_history = max_history
        self.current_index = -1

    def add_state(self, state):
        if self.current_index < len(self.history) - 1:
            self.history = self.history[:self.current_index + 1]
        self.history.append(state)
        if len(self.history) > self.max_history:
            self.history.pop(0)
        self.current_index = len(self.history) - 1

    def undo(self):
        if self.current_index > 0:
            self.current_index -= 1
            return self.history[self.current_index]
        return None

    def redo(self):
        if self.current_index < len(self.history) - 1:
            self.current_index += 1
            return self.history[self.current_index]
        return None

# core/model_wrapper.py
import torch
import torch.nn as nn
from typing import Dict, List, Any, Optional
import copy

class ModelWrapper(nn.Module):
    def __init__(self, model: nn.Module):
        super().__init__()
        self.model = model
        self.initial_state_dict = copy.deepcopy(model.state_dict())
        self.breakpoints: Dict[str, List[Dict[str, Any]]] = {}
        self.current_state: Dict[str, Any] = {}
        self.execution_paused = False
        self.step_mode = False
        self.current_step = 0
        self.current_layer = ""
        self.layer_order = []
        self.attention_weights = {}
        self.initial_state_dict = copy.deepcopy(model.state_dict())
        self.modified_weights = {}
        self.activation_hooks = {}
        self.register_hooks()

    def register_hooks(self):
        def attention_hook(module, input, output):
            self.attention_weights[module.__class__.__name__] = output[1] if isinstance(output, tuple) else output

        for name, module in self.model.named_modules():
            if "self_attn" in name:
                module.register_forward_hook(attention_hook)
            module.register_forward_hook(self.forward_hook(name))
            module.register_full_backward_hook(self.backward_hook(name))
            self.layer_order.append(name)

    def forward_hook(self, name):
        def hook(module, input, output):
            self.current_state[name] = {
                'input': input,
                'output': output,
                'parameters': {param_name: param.data for param_name, param in module.named_parameters()}
            }
            self.current_layer = name
            if name in self.breakpoints:
                for breakpoint in self.breakpoints[name]:
                    if breakpoint['condition'] is None or self._evaluate_condition(breakpoint['condition'], output):
                        self.execution_paused = True
                        print(f"Breakpoint hit at layer: {name}")
                        if breakpoint['condition']:
                            print(f"Condition satisfied: {breakpoint['condition']}")
                        return
        return hook

    def backward_hook(self, name):
        def hook(module, grad_input, grad_output):
            self.current_state[name]['grad_input'] = grad_input
            self.current_state[name]['grad_output'] = grad_output
            self.current_state[name]['grad_params'] = {param_name: param.grad for param_name, param in module.named_parameters()}
        return hook
    
    def _evaluate_condition(self, condition: str, output):
        try:
            return eval(condition, {'output': output, 'torch': torch})
        except Exception as e:
            print(f"Error evaluating breakpoint condition: {str(e)}")
            return False

    def forward(self, *args, **kwargs):
        return self.model(*args, **kwargs)


    def get_parameter(self, param_name: str) -> Optional[nn.Parameter]:
        return dict(self.model.named_parameters()).get(param_name)

    def set_breakpoint(self, layer_name: str, condition: Optional[str] = None, action: Optional[str] = None):
        if layer_name not in self.breakpoints:
            self.breakpoints[layer_name] = []
        self.breakpoints[layer_name].append({'condition': condition, 'action': action})

    def remove_breakpoint(self, layer_name: str):
        if layer_name in self.breakpoints:
            del self.breakpoints[layer_name]

    def clear_all_breakpoints(self):
        self.breakpoints.clear()

    def reset_to_initial_state(self):
        self.model.load_state_dict(self.initial_state_dict)
        print("Model reset to initial state.")

    def step_to_layer(self, target_layer: str):
        if not self.current_state:
            return "Error: No execution state. Run the model first."
        
        def find_layer(model, target):
            for name, child in model.named_children():
                if name == target:
                    return child
                result = find_layer(child, target)
                if result is not None:
                    return result
            return None

        target_module = find_layer(self.model, target_layer)
        if target_module is None:
            return f"Error: Layer {target_layer} not found"
    
    def get_attention_weights(self, layer_name: str):
        return self.attention_weights.get(layer_name)
    
    def modify_weight(self, layer_name: str, weight_name: str, indices, value):
        layer = self.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        if not hasattr(layer, weight_name):
            return f"Weight '{weight_name}' not found in layer '{layer_name}'."

        weight = getattr(layer, weight_name)
        if not isinstance(weight, torch.Tensor):
            return f"'{weight_name}' is not a tensor in layer '{layer_name}'."

        try:
            with torch.no_grad():
                original_value = weight[indices].clone()
                new_weight = weight.clone()
                new_weight[indices] = value
                setattr(layer, weight_name, nn.Parameter(new_weight))
                
            self.modified_weights[(layer_name, weight_name, indices)] = original_value
            return f"Weight at {layer_name}.{weight_name}{indices} modified to {value}"
        except Exception as e:
            return f"Error modifying weight: {str(e)}"

    def reset_modified_weights(self):
        with torch.no_grad():
            for (layer_name, weight_name, indices), original_value in self.modified_weights.items():
                layer = self.get_layer(layer_name)
                weight = getattr(layer, weight_name)
                new_weight = weight.clone()
                new_weight[indices] = original_value
                setattr(layer, weight_name, nn.Parameter(new_weight))
        self.modified_weights.clear()
        return "All modified weights have been reset."
    

    def modify_activation(self, layer_name: str, function_str: str):
        layer = self.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        try:
            modification_function = eval(f"lambda x: {function_str}")
        except Exception as e:
            return f"Error in function definition: {str(e)}"

        def hook(module, input, output):
            return modification_function(output)

        # Remove existing hook if there is one
        if layer_name in self.activation_hooks:
            self.activation_hooks[layer_name].remove()

        handle = layer.register_forward_hook(hook)
        self.activation_hooks[layer_name] = handle

        return f"Activation modification hook set for layer '{layer_name}'"

    def clear_activation_modifications(self):
        for handle in self.activation_hooks.values():
            handle.remove()
        self.activation_hooks.clear()
        return "All activation modifications cleared"

    def get_layer(self, layer_name: str) -> Optional[nn.Module]:
        parts = layer_name.split('.')
        current = self.model
        for part in parts:
            if hasattr(current, part):
                current = getattr(current, part)
            else:
                return None
        return current



    
    

# core/debugger.py
import torch
from typing import Any, Dict
from transformers import AutoTokenizer

from utils.error_handling import handle_exceptions
from .model_wrapper import ModelWrapper
from .execution_engine import ExecutionEngine
from inspection import ModelInspector, LayerInspector, WeightInspector, ActivationInspector, GradientInspector, AttentionInspector, VariableInspector
from breakpoints import BreakpointManager
from tracing import ExecutionTracer, ActivationTracer, GradientTracer
from analysis.token_probability import TokenProbabilityAnalyzer
from core.undo_manager import UndoManager
from advanced.custom_hooks import CustomHookManager

class NNGDB:
    def __init__(self, model: torch.nn.Module, model_name: str, device: str):
        self.wrapped_model = ModelWrapper(model)
        self.execution_engine = ExecutionEngine(self.wrapped_model)
        self.breakpoint_manager = BreakpointManager(self.wrapped_model)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.device = device
        # Inspectors
        self.model_inspector = ModelInspector(self.wrapped_model)
        self.layer_inspector = LayerInspector(self.wrapped_model)
        self.weight_inspector = WeightInspector(self.wrapped_model)
        self.activation_inspector = ActivationInspector(self.wrapped_model)
        self.gradient_inspector = GradientInspector(self.wrapped_model)
        self.attention_inspector = AttentionInspector(self.wrapped_model)
        self.variable_inspector = VariableInspector(self.wrapped_model)
        
        # Tracers
        self.execution_tracer = ExecutionTracer(self.wrapped_model)
        self.activation_tracer = ActivationTracer(self.wrapped_model)
        self.gradient_tracer = GradientTracer(self.wrapped_model)
        
        self.context: Dict[str, Any] = {}

        self.undo_manager = UndoManager()
        self.token_analyzer = TokenProbabilityAnalyzer(model, self.tokenizer)

        self.custom_hook_manager = CustomHookManager(self.wrapped_model.model)

    @handle_exceptions
    def analyze_token_probabilities(self, input_text, top_k=5):
        if self.tokenizer is None:
            return "Error: Tokenizer not initialized"
        try:
            result = self.token_analyzer.analyze(input_text, top_k)
            return f"Input: {input_text}\nTop {top_k} tokens:\n" + "\n".join([f"{token}: {prob:.4f}" for token, prob in result['top_tokens']])
        except Exception as e:
            return f"Error analyzing tokens: {str(e)}"

    @handle_exceptions
    def compare_token_probabilities(self, index1, index2):
        return self.token_analyzer.compare(index1, index2)
    
    @handle_exceptions
    def analyze_tokens(self, input_text: str, top_k: int = 5):
        if self.tokenizer is None:
            return "Error: Tokenizer not initialized"
        return self.token_analyzer.analyze(input_text, top_k)

    @handle_exceptions
    def undo(self):
        state = self.undo_manager.undo()
        if state:
            self.wrapped_model.load_state_dict(state)
            return "Undo successful"
        return "Nothing to undo"

    @handle_exceptions
    def redo(self):
        state = self.undo_manager.redo()
        if state:
            self.wrapped_model.load_state_dict(state)
            return "Redo successful"
        return "Nothing to redo"
    
    @handle_exceptions
    def run(self, input_text: str):
        try:
            # Tokenize input
            inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
            print(f"Input shape: {inputs.input_ids.shape}")

            # Generate output
            with torch.no_grad():
                outputs = self.wrapped_model.model(**inputs)

            print(f"Output type: {type(outputs)}")
            if hasattr(outputs, 'logits'):
                print(f"Logits shape: {outputs.logits.shape}")
            
            # Process output
            if hasattr(outputs, 'logits'):
                output_ids = outputs.logits[:, -1:].argmax(dim=-1)
            elif isinstance(outputs, tuple):
                output_ids = outputs[0][:, -1:]
            else:
                output_ids = outputs[:, -1:]

            print(f"Output IDs shape: {output_ids.shape}")

            # Decode output
            output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)

            return f"Input: {input_text}\nOutput: {output_text}"
        except Exception as e:
            return f"Error: {str(e)}\nError type: {type(e)}"
    
    @handle_exceptions
    def set_context(self, key: str, value: Any):
        self.context[key] = value

    @handle_exceptions
    def get_context(self, key: str) -> Any:
        return self.context.get(key)

    @handle_exceptions
    def inspect_model(self):
        model_info = self.model_inspector.inspect()
        return self._format_model_info(model_info)

    @handle_exceptions
    def _format_model_info(self, model_info):
        formatted = f"Model: {model_info['model_type']}\n"
        formatted += f"Total parameters: {model_info['num_parameters']}\n"
        formatted += f"Trainable parameters: {model_info['num_trainable_parameters']}\n"
        formatted += "Layers:\n"
        for name, layer_info in model_info['layers'].items():
            formatted += f"  {name}: {layer_info['type']}\n"
            for param_name, param_info in layer_info['parameters'].items():
                formatted += f"    {param_name}: shape={param_info['shape']}, requires_grad={param_info['requires_grad']}\n"
        return formatted
    
    @handle_exceptions
    def inspect_layer(self, layer_name: str):
        return self.layer_inspector.inspect(layer_name)

    @handle_exceptions
    def inspect_weights(self, layer_name: str):
        return self.weight_inspector.inspect(layer_name)
    
    @handle_exceptions
    def inspect_activations(self, layer_name: str):
        return self.activation_inspector.inspect(layer_name)

    @handle_exceptions
    def inspect_gradients(self, layer_name: str):
        return self.gradient_inspector.inspect(layer_name)

    @handle_exceptions
    def inspect_attention(self, layer_name: str):
        return self.attention_inspector.inspect(layer_name)

    @handle_exceptions
    def inspect_variable(self, variable_name: str):
        return self.variable_inspector.inspect(variable_name)

    @handle_exceptions
    def set_breakpoint(self, layer_name: str, condition: str = None):
        return self.breakpoint_manager.set_breakpoint(layer_name, condition)

    @handle_exceptions
    def remove_breakpoint(self, layer_name: str):
        return self.breakpoint_manager.remove_breakpoint(layer_name)

    @handle_exceptions
    def list_breakpoints(self):
        return self.breakpoint_manager.list_breakpoints()

    @handle_exceptions
    def trace_execution(self):
        return self.execution_tracer.trace()

    @handle_exceptions
    def trace_activations(self):
        return self.activation_tracer.trace()

    @handle_exceptions
    def trace_gradients(self):
        return self.gradient_tracer.trace()
    
    @handle_exceptions
    def get_activation_trace(self, layer_name: str):
        return self.activation_tracer.get_activation(layer_name)
    
    @handle_exceptions
    def get_execution_trace(self):
        return self.execution_tracer.get_trace()
    
    @handle_exceptions
    def clear_all_traces(self):
        self.execution_tracer.clear_trace()
        self.activation_tracer.clear_trace()
        self.gradient_tracer.clear_trace()
        return "All traces cleared."
    
    @handle_exceptions
    def step(self, num_steps: int = 1):
        return self.execution_engine.step(num_steps)
    
    @handle_exceptions
    def continue_execution(self):
        return self.execution_engine.continue_execution()
    
    @handle_exceptions
    def get_token_attention(self, layer_name: str, head_index: int):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."
        
        if not hasattr(layer, 'self_attn'):
            return f"Layer '{layer_name}' does not have attention weights."
        
        attention = layer.self_attn.attention_weights
        if attention is None:
            return f"No attention weights available for layer '{layer_name}'."
        
        if head_index >= attention.size(1):
            return f"Invalid head index. Layer '{layer_name}' has {attention.size(1)} attention heads."
        
        head_attention = attention[0, head_index].cpu().detach().numpy()
        return head_attention

    @handle_exceptions
    def get_token_representation(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No data available for layer '{layer_name}'."
        
        hidden_states = self.wrapped_model.current_state[layer_name]['output'][0]
        return hidden_states.cpu().detach().numpy()
    
    @handle_exceptions
    def get_gradient_trace(self, layer_name: str):
        return self.gradient_tracer.get_gradient(layer_name)
    
    @handle_exceptions
    def modify_weight(self, layer_name: str, weight_name: str, indices, value):
        return self.wrapped_model.modify_weight(layer_name, weight_name, indices, value)

    @handle_exceptions
    def modify_activation(self, layer_name: str, function_str: str):
        try:
            result = self.wrapped_model.modify_activation(layer_name, function_str)
            return result
        except Exception as e:
            return f"Error modifying activation: {str(e)}"

    @handle_exceptions
    def analyze_tokens_with_modified_weights(self, input_text, top_k=5):
        original_result = self.analyze_token_probabilities(input_text, top_k)
        
        # Ensure the input is on the correct device
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.wrapped_model.model(input_ids)
            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
        
        probs = torch.softmax(logits[0, -1], dim=-1)
        top_probs, top_indices = torch.topk(probs, top_k)
        
        modified_result = {
            "input_text": input_text,
            "top_tokens": [
                (self.tokenizer.decode([idx.item()]), prob.item())
                for idx, prob in zip(top_indices, top_probs)
            ]
        }
        
        comparison = f"Original analysis:\n{original_result}\n\n"
        comparison += f"Analysis with modified weights:\n"
        comparison += f"Input: {input_text}\nTop {top_k} tokens:\n" + "\n".join([f"{token}: {prob:.4f}" for token, prob in modified_result['top_tokens']])
        
        return comparison

    @handle_exceptions
    def reset_modified_weights(self):
        return self.wrapped_model.reset_modified_weights()

    @handle_exceptions  
    def get_layer_representation(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No data available for layer '{layer_name}'."
        return self.wrapped_model.current_state[layer_name]['output']
    
    @handle_exceptions
    def add_hook(self, hook_type: str, module_name: str, hook_name: str, hook_function: str):
        if hook_type == "forward":
            return self.custom_hook_manager.register_forward_hook(module_name, eval(hook_function), hook_name)
        elif hook_type == "backward":
            return self.custom_hook_manager.register_backward_hook(module_name, eval(hook_function), hook_name)
        else:
            return "Invalid hook type. Use 'forward' or 'backward'."
    
    @handle_exceptions
    def remove_hook(self, hook_name: str):
        return self.custom_hook_manager.remove_hook(hook_name)
    
    @handle_exceptions
    def list_hooks(self):
        return self.custom_hook_manager.list_hooks()

    @handle_exceptions
    def clear_hooks(self):
        return self.custom_hook_manager.clear_all_hooks()

# utils/data_generator.py
import torch

def generate_random_input(input_shape: tuple, device: str = 'cpu'):
    return torch.randn(input_shape).to(device)

def generate_adversarial_input(model: torch.nn.Module, original_input: torch.Tensor, target_class: int, epsilon: float = 0.01, num_steps: int = 10):
    perturbed_input = original_input.clone().detach().requires_grad_(True)
    
    for _ in range(num_steps):
        output = model(perturbed_input)
        loss = -output[0, target_class]
        loss.backward()
        
        with torch.no_grad():
            perturbed_input += epsilon * perturbed_input.grad.sign()
            perturbed_input.clamp_(0, 1)  # Assuming input values are between 0 and 1
        
        perturbed_input.grad.zero_()
    
    return perturbed_input.detach()

# utils/__init.py
from .tensor_utils import *
from .data_generator import *
from .performance_utils import *

__all__ = [
    'tensor_stats',
    'tensor_histogram',
    'generate_random_input',
    'generate_adversarial_input',
    'measure_inference_time',
    'profile_memory_usage'
]

# utils/tensor_utils.py
import torch

def tensor_stats(tensor: torch.Tensor):
    return {
        "shape": tensor.shape,
        "mean": tensor.mean().item(),
        "std": tensor.std().item(),
        "min": tensor.min().item(),
        "max": tensor.max().item(),
        "norm": tensor.norm().item(),
    }

def tensor_histogram(tensor: torch.Tensor, num_bins: int = 10):
    hist = torch.histogram(tensor.float().view(-1), bins=num_bins)
    return {
        "bin_edges": hist.bin_edges.tolist(),
        "counts": hist.count.tolist()
    }

# utils/performance_utils.py
import time
import torch

def measure_inference_time(model: torch.nn.Module, input_tensor: torch.Tensor, num_runs: int = 100):
    start_time = time.time()
    
    with torch.no_grad():
        for _ in range(num_runs):
            _ = model(input_tensor)
    
    end_time = time.time()
    avg_time = (end_time - start_time) / num_runs
    
    return avg_time

def profile_memory_usage(model: torch.nn.Module, input_tensor: torch.Tensor):
    initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
    
    output = model(input_tensor)
    
    final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
    memory_usage = final_memory - initial_memory
    
    return memory_usage

# utils/error_handling.py
class NNGDBException(Exception):
    pass

def handle_exceptions(func):
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except NNGDBException as e:
            return f"Error: {str(e)}"
        except Exception as e:
            return f"Unexpected error: {str(e)}"
    return wrapper

# utils/activation_utils.py
import torch

def analyze_activation(activation: torch.Tensor):
    return {
        "shape": activation.shape,
        "mean": activation.mean().item(),
        "std": activation.std().item(),
        "min": activation.min().item(),
        "max": activation.max().item(),
        "fraction_zeros": (activation == 0).float().mean().item(),
    }

# advanced/__init__.py
from .custom_hooks import CustomHookManager
from .interpretability_metrics import InterpretabilityMetrics
from .explainability_techniques import ExplainabilityTechniques

__all__ = ['CustomHookManager', 'InterpretabilityMetrics', 'ExplainabilityTechniques']

# advanced/interpretability_metrics.py
import torch
import torch.nn.functional as F

class InterpretabilityMetrics:
    @staticmethod
    def compute_activation_stability(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, layer_name: str):
        activations = []
        
        def hook(module, input, output):
            activations.append(output.detach())
        
        for name, module in model.named_modules():
            if name == layer_name:
                handle = module.register_forward_hook(hook)
                break
        else:
            return f"Layer '{layer_name}' not found"
        
        model.eval()
        with torch.no_grad():
            for batch in dataloader:
                inputs = batch[0]
                model(inputs)
        
        handle.remove()
        
        activations = torch.cat(activations, dim=0)
        stability = torch.std(activations, dim=0).mean().item()
        
        return f"Activation stability for layer '{layer_name}': {stability:.4f}"

    @staticmethod
    def compute_loss_sensitivity(model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor, epsilon: float = 1e-4):
        model.eval()
        inputs.requires_grad_(True)
        
        original_loss = F.cross_entropy(model(inputs), targets)
        original_loss.backward()
        
        input_grad = inputs.grad.data
        
        perturbed_inputs = inputs + epsilon * input_grad.sign()
        perturbed_inputs = torch.clamp(perturbed_inputs, 0, 1)
        
        with torch.no_grad():
            perturbed_loss = F.cross_entropy(model(perturbed_inputs), targets)
        
        sensitivity = (perturbed_loss - original_loss) / epsilon
        
        return f"Loss sensitivity: {sensitivity.item():.4f}"

    @staticmethod
    def compute_decision_boundary_distance(model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor, num_steps: int = 100, step_size: float = 0.01):
        model.eval()
        inputs.requires_grad_(True)
        
        for _ in range(num_steps):
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            
            with torch.no_grad():
                inputs -= step_size * inputs.grad.sign()
                inputs.clamp_(0, 1)
            
            inputs.grad.zero_()
            
            predicted = outputs.argmax(dim=1)
            if (predicted != targets).any():
                break
        
        distance = (inputs - inputs.data).norm(dim=1).mean()
        return f"Average distance to decision boundary: {distance.item():.4f}"

# advanced/explainability_techniques.py
import torch
import torch.nn.functional as F
import numpy as np

class ExplainabilityTechniques:
    @staticmethod
    def compute_saliency_map(model: torch.nn.Module, inputs: torch.Tensor, target_class: int):
        model.eval()
        inputs.requires_grad_(True)
        
        outputs = model(inputs)
        score = outputs[:, target_class].sum()
        score.backward()
        
        saliency, _ = torch.max(inputs.grad.data.abs(), dim=1)
        return saliency

    @staticmethod
    def integrated_gradients(model: torch.nn.Module, inputs: torch.Tensor, target_class: int, steps: int = 100):
        model.eval()
        inputs.requires_grad_(True)
        
        baseline = torch.zeros_like(inputs)
        scaled_inputs = [baseline + (float(i) / steps) * (inputs - baseline) for i in range(steps + 1)]
        grads = []
        
        for scaled_input in scaled_inputs:
            scaled_input.requires_grad_(True)
            outputs = model(scaled_input)
            score = outputs[:, target_class].sum()
            score.backward()
            grads.append(scaled_input.grad.clone())
            scaled_input.grad.zero_()
        
        avg_grads = torch.cat(grads).mean(dim=0)
        integrated_grad = (inputs - baseline) * avg_grads
        return integrated_grad

    @staticmethod
    def occlusion_sensitivity(model: torch.nn.Module, inputs: torch.Tensor, target_class: int, window_size: int = 8):
        model.eval()
        batch_size, channels, height, width = inputs.shape
        occlusion_map = torch.zeros((height, width))
        
        original_score = model(inputs)[:, target_class].item()
        
        for i in range(0, height, window_size):
            for j in range(0, width, window_size):
                occluded_input = inputs.clone()
                occluded_input[:, :, i:i+window_size, j:j+window_size] = 0
                occluded_score = model(occluded_input)[:, target_class].item()
                occlusion_map[i:i+window_size, j:j+window_size] = original_score - occluded_score
        
        return occlusion_map

    @staticmethod
    def grad_cam(model: torch.nn.Module, inputs: torch.Tensor, target_class: int, layer_name: str):
        model.eval()
        
        # Get the specified layer
        for name, module in model.named_modules():
            if name == layer_name:
                target_layer = module
                break
        else:
            raise ValueError(f"Layer {layer_name} not found in the model")
        
        # Hook for getting layer output and gradients
        layer_output = None
        layer_grad = None
        
        def save_output_and_grad(module, input, output):
            nonlocal layer_output, layer_grad
            layer_output = output
            output.register_hook(lambda grad: setattr(grad, 'grad', grad))
        
        handle = target_layer.register_forward_hook(save_output_and_grad)
        
        # Forward pass
        model_output = model(inputs)
        model_output[0, target_class].backward()
        
        handle.remove()
        
        # Compute GradCAM
        gradients = layer_output.grad
        pooled_gradients = torch.mean(gradients, dim=[2, 3], keepdim=True)
        cam = torch.sum(layer_output * pooled_gradients, dim=1, keepdim=True)
        cam = F.relu(cam)
        cam = F.interpolate(cam, size=inputs.shape[2:], mode='bilinear', align_corners=False)
        cam = cam / cam.max()
        
        return cam.squeeze()

# advanced/custom_hooks.py
import torch
from typing import Callable, Dict, Any

class CustomHookManager:
    def __init__(self, model: torch.nn.Module):
        self.model = model
        self.hooks: Dict[str, Any] = {}

    def register_forward_hook(self, module_name: str, hook: Callable, name: str):
        module = dict(self.model.named_modules())[module_name]
        handle = module.register_forward_hook(hook)
        self.hooks[name] = handle
        return f"Forward hook '{name}' registered for module '{module_name}'"

    def register_backward_hook(self, module_name: str, hook: Callable, name: str):
        module = dict(self.model.named_modules())[module_name]
        handle = module.register_full_backward_hook(hook)
        self.hooks[name] = handle
        return f"Backward hook '{name}' registered for module '{module_name}'"

    def remove_hook(self, name: str):
        if name in self.hooks:
            self.hooks[name].remove()
            del self.hooks[name]
            return f"Hook '{name}' removed"
        return f"Hook '{name}' not found"

    def clear_all_hooks(self):
        for handle in self.hooks.values():
            handle.remove()
        self.hooks.clear()
        return "All hooks cleared"

    def list_hooks(self):
        return "\n".join(f"{name}: {type(hook).__name__}" for name, hook in self.hooks.items())

# tracing/__init__.py
from .execution_tracer import ExecutionTracer
from .activation_tracer import ActivationTracer
from .gradient_tracer import GradientTracer

__all__ = ['ExecutionTracer', 'ActivationTracer', 'GradientTracer']

# tracing/execution_tracer.py
from core.model_wrapper import ModelWrapper

class ExecutionTracer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model
        self.execution_trace = []

    def trace(self):
        self.execution_trace = []
        self._register_hooks()
        return "Execution tracing enabled. Run the model to collect the trace."

    def _register_hooks(self):
        def hook(module, input, output):
            self.execution_trace.append({
                'layer_name': module.__class__.__name__,
                'input_shape': [tuple(i.shape) for i in input if hasattr(i, 'shape')],
                'output_shape': tuple(output.shape) if hasattr(output, 'shape') else None
            })

        for name, module in self.wrapped_model.model.named_modules():
            module.register_forward_hook(hook)

    def get_trace(self):
        return self.execution_trace

    def clear_trace(self):
        self.execution_trace = []
        return "Execution trace cleared."

    def summarize_trace(self):
        summary = []
        for idx, step in enumerate(self.execution_trace):
            summary.append(f"Step {idx}: {step['layer_name']} - Input: {step['input_shape']}, Output: {step['output_shape']}")
        return "\n".join(summary)

# tracing/gradient_tracer.py
import torch
from core.model_wrapper import ModelWrapper

class GradientTracer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model
        self.gradient_trace = {}

    def trace(self):
        self.gradient_trace = {}
        self._register_hooks()
        return "Gradient tracing enabled. Run backward pass to collect gradients."

    def _register_hooks(self):
        def hook(module, grad_input, grad_output):
            self.gradient_trace[module.__class__.__name__] = {
                'grad_input': [g.detach() if isinstance(g, torch.Tensor) else g for g in grad_input],
                'grad_output': [g.detach() if isinstance(g, torch.Tensor) else g for g in grad_output]
            }

        for name, module in self.wrapped_model.model.named_modules():
            module.register_full_backward_hook(hook)

    def get_gradient(self, layer_name: str):
        return self.gradient_trace.get(layer_name, f"No gradient recorded for layer '{layer_name}'")

    def get_all_gradients(self):
        return self.gradient_trace

    def clear_trace(self):
        self.gradient_trace = {}
        return "Gradient trace cleared."

    def summarize_trace(self):
        summary = []
        for layer_name, grads in self.gradient_trace.items():
            summary.append(f"Layer: {layer_name}")
            for grad_type in ['grad_input', 'grad_output']:
                for idx, grad in enumerate(grads[grad_type]):
                    if isinstance(grad, torch.Tensor):
                        summary.append(f"  {grad_type}[{idx}]: shape={grad.shape}, mean={grad.mean().item():.4f}, std={grad.std().item():.4f}")
                    else:
                        summary.append(f"  {grad_type}[{idx}]: {type(grad)}")
        return "\n".join(summary)

# tracing/activation_tracer.py
import torch
from core.model_wrapper import ModelWrapper

class ActivationTracer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model
        self.activation_trace = {}

    def trace(self):
        def hook(module, input, output):
            self.activation_trace[module.__class__.__name__] = output.detach()

        for name, module in self.wrapped_model.model.named_modules():
            module.register_forward_hook(hook)

    def _register_hooks(self):
        def hook(module, input, output):
            self.activation_trace[module.__class__.__name__] = output.detach() if isinstance(output, torch.Tensor) else output

        for name, module in self.wrapped_model.model.named_modules():
            module.register_forward_hook(hook)

    def get_activation(self, layer_name: str):
        return self.activation_trace.get(layer_name, f"No activation recorded for layer '{layer_name}'")

    def get_all_activations(self):
        return self.activation_trace

    def clear_trace(self):
        self.activation_trace = {}
        return "Activation trace cleared."

    def summarize_trace(self):
        summary = []
        for layer_name, activation in self.activation_trace.items():
            if isinstance(activation, torch.Tensor):
                summary.append(f"{layer_name}: shape={activation.shape}, mean={activation.mean().item():.4f}, std={activation.std().item():.4f}")
            else:
                summary.append(f"{layer_name}: {type(activation)}")
        return "\n".join(summary)

# analysis/__init__.py
from .gradient_flow import GradientFlowAnalyzer
from .attention_analysis import AttentionAnalyzer
from .neuron_activation import NeuronActivationAnalyzer
from .perturbation_analysis import PerturbationAnalyzer
from .token_probability import TokenProbabilityAnalyzer

__all__ = ['GradientFlowAnalyzer', 'AttentionAnalyzer', 'NeuronActivationAnalyzer', 'PerturbationAnalyzer', 'TokenProbabilityAnalyzer']

# analysis/perturbation_analysis.py
import torch
from core.model_wrapper import ModelWrapper

class PerturbationAnalyzer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def analyze_input_perturbation(self, input_tensor: torch.Tensor, perturbation_scale: float = 0.1, num_samples: int = 10):
        original_output = self.wrapped_model.model(input_tensor)
        
        perturbed_outputs = []
        for _ in range(num_samples):
            perturbation = torch.randn_like(input_tensor) * perturbation_scale
            perturbed_input = input_tensor + perturbation
            perturbed_output = self.wrapped_model.model(perturbed_input)
            perturbed_outputs.append(perturbed_output)

        output_diff = torch.stack([output - original_output for output in perturbed_outputs])
        
        analysis = {
            "mean_output_diff": output_diff.abs().mean().item(),
            "std_output_diff": output_diff.std().item(),
            "max_output_diff": output_diff.abs().max().item(),
        }

        return analysis

    def analyze_weight_perturbation(self, layer_name: str, perturbation_scale: float = 0.1, num_samples: int = 10):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        original_weights = {name: param.clone() for name, param in layer.named_parameters() if 'weight' in name}
        original_output = self.wrapped_model.model(self.wrapped_model.current_state['input'])

        perturbed_outputs = []
        for _ in range(num_samples):
            with torch.no_grad():
                for name, param in layer.named_parameters():
                    if 'weight' in name:
                        perturbation = torch.randn_like(param) * perturbation_scale
                        param.add_(perturbation)
                
                perturbed_output = self.wrapped_model.model(self.wrapped_model.current_state['input'])
                perturbed_outputs.append(perturbed_output)

                # Restore original weights
                for name, original_weight in original_weights.items():
                    getattr(layer, name).copy_(original_weight)

        output_diff = torch.stack([output - original_output for output in perturbed_outputs])

        analysis = {
            "mean_output_diff": output_diff.abs().mean().item(),
            "std_output_diff": output_diff.std().item(),
            "max_output_diff": output_diff.abs().max().item(),
        }

        return analysis

    def compute_input_gradient(self, input_tensor: torch.Tensor):
        input_tensor.requires_grad_(True)
        output = self.wrapped_model.model(input_tensor)
        
        if isinstance(output, torch.Tensor):
            loss = output.sum()
        elif isinstance(output, tuple):
            loss = output[0].sum()  # Assuming the first element is the main output
        else:
            return "Unexpected output type. Unable to compute input gradient."

        loss.backward()

        input_gradient = input_tensor.grad

        return {
            "input_gradient_norm": input_gradient.norm().item(),
            "input_gradient_mean": input_gradient.abs().mean().item(),
            "input_gradient_std": input_gradient.std().item(),
        }

    def compute_saliency_map(self, input_tensor: torch.Tensor):
        input_tensor.requires_grad_(True)
        output = self.wrapped_model.model(input_tensor)
        
        if isinstance(output, torch.Tensor):
            loss = output.sum()
        elif isinstance(output, tuple):
            loss = output[0].sum()
        else:
            return "Unexpected output type. Unable to compute saliency map."

        loss.backward()

        saliency_map = input_tensor.grad.abs()
        
        return {
            "saliency_map": saliency_map,
            "max_saliency": saliency_map.max().item(),
            "mean_saliency": saliency_map.mean().item(),
        }

# analysis/neuron_activation.py
import torch
from core.model_wrapper import ModelWrapper

class NeuronActivationAnalyzer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def analyze(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No activation data available for layer '{layer_name}'."

        activation = self.wrapped_model.current_state[layer_name]['output']
        if not isinstance(activation, torch.Tensor):
            return f"Activation for layer '{layer_name}' is not a tensor."

        return self._analyze_activation(activation)

    def _analyze_activation(self, activation: torch.Tensor):
        analysis = {
            "shape": activation.shape,
            "mean": activation.mean().item(),
            "std": activation.std().item(),
            "min": activation.min().item(),
            "max": activation.max().item(),
            "fraction_zeros": (activation == 0).float().mean().item(),
            "top_k_active": self._get_top_k_active(activation, k=10),
            "activation_statistics": self._compute_activation_statistics(activation),
        }
        return analysis

    def _get_top_k_active(self, activation: torch.Tensor, k: int):
        if activation.dim() > 2:
            activation = activation.view(activation.size(0), -1)
        top_k_values, top_k_indices = torch.topk(activation.abs().mean(dim=0), k)
        return [(idx.item(), val.item()) for idx, val in zip(top_k_indices, top_k_values)]

    def _compute_activation_statistics(self, activation: torch.Tensor):
        if activation.dim() > 2:
            activation = activation.view(activation.size(0), -1)
        
        positive_fraction = (activation > 0).float().mean(dim=0)
        negative_fraction = (activation < 0).float().mean(dim=0)
        zero_fraction = (activation == 0).float().mean(dim=0)

        return {
            "positive_fraction": positive_fraction.mean().item(),
            "negative_fraction": negative_fraction.mean().item(),
            "zero_fraction": zero_fraction.mean().item(),
        }

    def get_most_active_neurons(self, layer_name: str, k: int = 10):
        if layer_name not in self.wrapped_model.current_state:
            return f"No activation data available for layer '{layer_name}'."

        activation = self.wrapped_model.current_state[layer_name]['output']
        if not isinstance(activation, torch.Tensor):
            return f"Activation for layer '{layer_name}' is not a tensor."

        if activation.dim() > 2:
            activation = activation.view(activation.size(0), -1)

        mean_activation = activation.abs().mean(dim=0)
        top_k_values, top_k_indices = torch.topk(mean_activation, k)

        return [(idx.item(), val.item()) for idx, val in zip(top_k_indices, top_k_values)]

    def compute_activation_similarity(self, layer_name: str, reference_input: torch.Tensor):
        if layer_name not in self.wrapped_model.current_state:
            return f"No activation data available for layer '{layer_name}'."

        current_activation = self.wrapped_model.current_state[layer_name]['output']
        if not isinstance(current_activation, torch.Tensor):
            return f"Activation for layer '{layer_name}' is not a tensor."

        with torch.no_grad():
            reference_activation = self.wrapped_model.model(reference_input)
            reference_activation = self.wrapped_model.current_state[layer_name]['output']

        similarity = torch.nn.functional.cosine_similarity(current_activation.view(-1), reference_activation.view(-1), dim=0)

        return similarity.item()

# analysis/token_probability.py
import torch

class TokenProbabilityAnalyzer:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
        self.history = []

    def analyze(self, input_text, top_k=5):
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
        
        probs = torch.softmax(logits[0, -1], dim=-1)
        top_probs, top_indices = torch.topk(probs, top_k)
        
        result = {
            "input_text": input_text,
            "top_tokens": [
                (self.tokenizer.decode([idx.item()]), prob.item())
                for idx, prob in zip(top_indices, top_probs)
            ]
        }
        self.history.append(result)
        return result

    def compare(self, index1, index2):
        if index1 >= len(self.history) or index2 >= len(self.history):
            return f"Invalid indices for comparison. Available range: 0-{len(self.history)-1}"
        
        result1 = self.history[index1]
        result2 = self.history[index2]
        
        comparison = f"Comparison:\n"
        comparison += f"Input 1: {result1['input_text']}\n"
        comparison += f"Input 2: {result2['input_text']}\n\n"
        comparison += "Top tokens:\n"
        
        for (token1, prob1), (token2, prob2) in zip(result1['top_tokens'], result2['top_tokens']):
            comparison += f"{token1} ({prob1:.4f}) vs {token2} ({prob2:.4f})\n"
        
        return comparison

# analysis/gradient_flow.py
import torch
from core.model_wrapper import ModelWrapper

class GradientFlowAnalyzer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def analyze(self):
        gradients = {}
        for name, param in self.wrapped_model.model.named_parameters():
            if param.requires_grad and param.grad is not None:
                gradients[name] = {
                    'mean': param.grad.abs().mean().item(),
                    'std': param.grad.std().item(),
                    'max': param.grad.abs().max().item(),
                    'norm': param.grad.norm().item()
                }

        return gradients

    def detect_vanishing_exploding_gradients(self, threshold=1e-4):
        vanishing = []
        exploding = []
        for name, param in self.wrapped_model.model.named_parameters():
            if param.requires_grad and param.grad is not None:
                grad_norm = param.grad.norm().item()
                if grad_norm < threshold:
                    vanishing.append((name, grad_norm))
                elif grad_norm > 1/threshold:
                    exploding.append((name, grad_norm))
        
        return {
            'vanishing': vanishing,
            'exploding': exploding
        }

    def compute_layer_gradients(self):
        layer_gradients = {}
        for name, module in self.wrapped_model.model.named_modules():
            if list(module.children()):  # skip container modules
                continue
            layer_grad = torch.cat([p.grad.view(-1) for p in module.parameters() if p.grad is not None])
            layer_gradients[name] = {
                'mean': layer_grad.abs().mean().item(),
                'std': layer_grad.std().item(),
                'max': layer_grad.abs().max().item(),
                'norm': layer_grad.norm().item()
            }
        return layer_gradients

# analysis/attention_analysis.py
import torch
from core.model_wrapper import ModelWrapper

class AttentionAnalyzer:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def analyze_attention_patterns(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No attention data available for layer '{layer_name}'."

        layer_output = self.wrapped_model.current_state[layer_name]['output']
        
        if isinstance(layer_output, tuple):
            attention_weights = layer_output[1]
        elif isinstance(layer_output, torch.Tensor):
            attention_weights = layer_output
        else:
            return f"Unexpected output type for layer '{layer_name}': {type(layer_output)}"

        return self._analyze_attention(attention_weights)

    def _analyze_attention(self, attention_weights: torch.Tensor):
        batch_size, num_heads, seq_len, _ = attention_weights.shape

        avg_attention = attention_weights.mean(dim=(0, 1))  # Average over batch and heads
        
        analysis = {
            "shape": attention_weights.shape,
            "num_heads": num_heads,
            "sequence_length": seq_len,
            "entropy": self._compute_attention_entropy(attention_weights),
            "top_k_attention": self._get_top_k_attention(avg_attention, k=5),
            "attention_to_self": self._compute_attention_to_self(avg_attention),
            "attention_to_neighbors": self._compute_attention_to_neighbors(avg_attention),
        }

        return analysis

    def _compute_attention_entropy(self, attention_weights: torch.Tensor):
        attention_probs = attention_weights.mean(dim=1)  # Average over heads
        entropy = -(attention_probs * torch.log(attention_probs + 1e-9)).sum(dim=-1).mean().item()
        return entropy

    def _get_top_k_attention(self, avg_attention: torch.Tensor, k: int):
        top_k_values, top_k_indices = torch.topk(avg_attention.mean(dim=0), k)
        return [(idx.item(), val.item()) for idx, val in zip(top_k_indices, top_k_values)]

    def _compute_attention_to_self(self, avg_attention: torch.Tensor):
        return torch.diag(avg_attention).mean().item()

    def _compute_attention_to_neighbors(self, avg_attention: torch.Tensor):
        seq_len = avg_attention.shape[0]
        neighbor_attention = torch.diag(avg_attention, diagonal=1) + torch.diag(avg_attention, diagonal=-1)
        return neighbor_attention.sum().item() / (2 * (seq_len - 1))

    def visualize_attention(self, layer_name: str):
        # This method would typically create a visualization of the attention weights.
        # For now, we'll just return a message indicating that visualization is not implemented.
        return "Attention visualization not implemented in this version."

# inspection/__init__.py
from .model_inspector import ModelInspector
from .layer_inspector import LayerInspector
from .weight_inspector import WeightInspector
from .activation_inspector import ActivationInspector
from .gradient_inspector import GradientInspector
from .attention_inspector import AttentionInspector
from .variable_inspector import VariableInspector

__all__ = [
    'ModelInspector',
    'LayerInspector',
    'WeightInspector',
    'ActivationInspector',
    'GradientInspector',
    'AttentionInspector',
    'VariableInspector'
]

# inspection/weight_inspector.py
import torch
from core.model_wrapper import ModelWrapper

class WeightInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self, layer_name: str):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        weight_info = {}
        for name, param in layer.named_parameters():
            if 'weight' in name:
                weight_info[name] = self._analyze_weight(param)
        return weight_info

    def _analyze_weight(self, weight: torch.Tensor):
        return {
            "shape": weight.shape,
            "mean": weight.mean().item(),
            "std": weight.std().item(),
            "min": weight.min().item(),
            "max": weight.max().item(),
            "norm": weight.norm().item(),
            "num_zeros": (weight == 0).sum().item(),
            "num_non_zeros": (weight != 0).sum().item(),
        }

    def get_weight(self, layer_name: str, weight_name: str):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."
        
        for name, param in layer.named_parameters():
            if name == weight_name:
                return param
        return f"Weight '{weight_name}' not found in layer '{layer_name}'."

# inspection/activation_inspector.py
import torch
from core.model_wrapper import ModelWrapper

class ActivationInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No activation data available for layer '{layer_name}'."

        activation = self.wrapped_model.current_state[layer_name]['output']
        if not isinstance(activation, torch.Tensor):
            return f"Activation for layer '{layer_name}' is not a tensor."

        return self._analyze_activation(activation)

    def _analyze_activation(self, activation: torch.Tensor):
        return {
            "shape": activation.shape,
            "mean": activation.mean().item(),
            "std": activation.std().item(),
            "min": activation.min().item(),
            "max": activation.max().item(),
            "num_zeros": (activation == 0).sum().item(),
            "num_non_zeros": (activation != 0).sum().item(),
            "fraction_zeros": ((activation == 0).sum() / activation.numel()).item(),
        }

    def get_activation(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No activation data available for layer '{layer_name}'."
        return self.wrapped_model.current_state[layer_name]['output']

# inspection/variable_inspector.py
from core.model_wrapper import ModelWrapper

class VariableInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self, variable_name: str):
        parts = variable_name.split('.')
        current = self.wrapped_model.model

        try:
            for part in parts:
                if hasattr(current, part):
                    current = getattr(current, part)
                else:
                    return f"Variable '{variable_name}' not found."

            return self._analyze_variable(current)
        except Exception as e:
            return f"Error inspecting variable '{variable_name}': {str(e)}"

    def _analyze_variable(self, variable):
        import torch

        if isinstance(variable, torch.Tensor):
            return {
                "type": "Tensor",
                "shape": variable.shape,
                "dtype": str(variable.dtype),
                "device": str(variable.device),
                "requires_grad": variable.requires_grad,
                "is_leaf": variable.is_leaf,
                "mean": variable.mean().item(),
                "std": variable.std().item(),
                "min": variable.min().item(),
                "max": variable.max().item(),
            }
        elif isinstance(variable, torch.nn.Parameter):
            return {
                "type": "Parameter",
                "shape": variable.shape,
                "dtype": str(variable.dtype),
                "device": str(variable.device),
                "requires_grad": variable.requires_grad,
                "is_leaf": variable.is_leaf,
                "mean": variable.mean().item(),
                "std": variable.std().item(),
                "min": variable.min().item(),
                "max": variable.max().item(),
            }
        else:
            return {
                "type": type(variable).__name__,
                "value": str(variable),
            }

    def get_variable(self, variable_name: str):
        parts = variable_name.split('.')
        current = self.wrapped_model.model

        for part in parts:
            if hasattr(current, part):
                current = getattr(current, part)
            else:
                return f"Variable '{variable_name}' not found."

        return current

# inspection/model_inspector.py
from core.model_wrapper import ModelWrapper

class ModelInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self):
        model_info = {
            "model_type": type(self.wrapped_model.model).__name__,
            "num_parameters": sum(p.numel() for p in self.wrapped_model.model.parameters()),
            "num_trainable_parameters": sum(p.numel() for p in self.wrapped_model.model.parameters() if p.requires_grad),
            "layers": self._get_layers_info()
        }
        return model_info

    def _get_layers_info(self):
        layers_info = {}
        for name, module in self.wrapped_model.model.named_modules():
            if not list(module.children()):  # Only leaf modules
                layers_info[name] = {
                    "type": type(module).__name__,
                    "parameters": {
                        param_name: {
                            "shape": param.shape,
                            "requires_grad": param.requires_grad
                        } for param_name, param in module.named_parameters()
                    }
                }
        return layers_info

# inspection/gradient_inspector.py
import torch
from core.model_wrapper import ModelWrapper

class GradientInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No gradient data available for layer '{layer_name}'."

        grad_info = {}
        if 'grad_input' in self.wrapped_model.current_state[layer_name]:
            grad_input = self.wrapped_model.current_state[layer_name]['grad_input']
            grad_info['input_gradient'] = self._analyze_gradient(grad_input)

        if 'grad_output' in self.wrapped_model.current_state[layer_name]:
            grad_output = self.wrapped_model.current_state[layer_name]['grad_output']
            grad_info['output_gradient'] = self._analyze_gradient(grad_output)

        if 'grad_params' in self.wrapped_model.current_state[layer_name]:
            grad_params = self.wrapped_model.current_state[layer_name]['grad_params']
            grad_info['parameter_gradients'] = {name: self._analyze_gradient(grad) for name, grad in grad_params.items()}

        return grad_info

    def _analyze_gradient(self, gradient):
        if not isinstance(gradient, torch.Tensor):
            return "Gradient is not a tensor."

        return {
            "shape": gradient.shape,
            "mean": gradient.mean().item(),
            "std": gradient.std().item(),
            "min": gradient.min().item(),
            "max": gradient.max().item(),
            "norm": gradient.norm().item(),
            "num_zeros": (gradient == 0).sum().item(),
            "num_non_zeros": (gradient != 0).sum().item(),
        }

    def get_gradient(self, layer_name: str, grad_type: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No gradient data available for layer '{layer_name}'."
        
        if grad_type == 'input':
            return self.wrapped_model.current_state[layer_name].get('grad_input')
        elif grad_type == 'output':
            return self.wrapped_model.current_state[layer_name].get('grad_output')
        elif grad_type == 'params':
            return self.wrapped_model.current_state[layer_name].get('grad_params')
        else:
            return f"Invalid gradient type '{grad_type}'. Choose from 'input', 'output', or 'params'."

# inspection/layer_inspector.py
import torch
from core.model_wrapper import ModelWrapper

class LayerInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self, layer_name: str):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        layer_info = {
            "name": layer_name,
            "type": type(layer).__name__,
            "parameters": self._get_parameters_info(layer),
            "input_shape": self._get_input_shape(layer_name),
            "output_shape": self._get_output_shape(layer_name)
        }
        return layer_info

    def _get_parameters_info(self, layer):
        return {name: {"shape": param.shape, "requires_grad": param.requires_grad}
                for name, param in layer.named_parameters()}

    def _get_input_shape(self, layer_name):
        if layer_name in self.wrapped_model.current_state:
            inputs = self.wrapped_model.current_state[layer_name]['input']
            return [tuple(input.shape) for input in inputs]
        return None

    def _get_output_shape(self, layer_name):
        if layer_name in self.wrapped_model.current_state:
            output = self.wrapped_model.current_state[layer_name]['output']
            return tuple(output.shape) if isinstance(output, torch.Tensor) else None
        return None

# inspection/attention_inspector.py
import torch
from core.model_wrapper import ModelWrapper

class AttentionInspector:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def inspect(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No attention data available for layer '{layer_name}'."

        layer_output = self.wrapped_model.current_state[layer_name]['output']
        
        # Check if the layer output is a tuple (common in transformer models)
        if isinstance(layer_output, tuple):
            # Assume the second element contains attention weights
            attention_weights = layer_output[1]
        elif isinstance(layer_output, torch.Tensor):
            # If it's a tensor, assume it's the attention weights directly
            attention_weights = layer_output
        else:
            return f"Unexpected output type for layer '{layer_name}': {type(layer_output)}"

        return self._analyze_attention(attention_weights)

    def _analyze_attention(self, attention_weights: torch.Tensor):
        if len(attention_weights.shape) != 4:
            return f"Unexpected shape for attention weights: {attention_weights.shape}"

        batch_size, num_heads, seq_len, _ = attention_weights.shape

        return {
            "shape": attention_weights.shape,
            "num_heads": num_heads,
            "sequence_length": seq_len,
            "mean": attention_weights.mean().item(),
            "std": attention_weights.std().item(),
            "min": attention_weights.min().item(),
            "max": attention_weights.max().item(),
            "entropy": self._compute_attention_entropy(attention_weights),
            "top_k_attention": self._get_top_k_attention(attention_weights, k=5)
        }

    def _compute_attention_entropy(self, attention_weights: torch.Tensor):
        # Compute entropy of attention distribution
        attention_probs = attention_weights.mean(dim=1)  # Average over heads
        entropy = -(attention_probs * torch.log(attention_probs + 1e-9)).sum(dim=-1).mean().item()
        return entropy

    def _get_top_k_attention(self, attention_weights: torch.Tensor, k: int):
        # Get top-k attended positions
        mean_attention = attention_weights.mean(dim=(0, 1))  # Average over batch and heads
        top_k_values, top_k_indices = torch.topk(mean_attention, k)
        return [(idx.item(), val.item()) for idx, val in zip(top_k_indices, top_k_values)]

    def get_attention_weights(self, layer_name: str):
        if layer_name not in self.wrapped_model.current_state:
            return f"No attention data available for layer '{layer_name}'."
        
        layer_output = self.wrapped_model.current_state[layer_name]['output']
        
        if isinstance(layer_output, tuple):
            return layer_output[1]
        elif isinstance(layer_output, torch.Tensor):
            return layer_output
        else:
            return f"Unexpected output type for layer '{layer_name}': {type(layer_output)}"

# cli/__init__.py
from .repl import NNGDBREPL
from .command_handler import CommandHandler
from .python_repl import PythonREPL

__all__ = ['NNGDBREPL', 'CommandHandler', 'PythonREPL']

# cli/repl.py
import readline
import rlcompleter
from .command_handler import CommandHandler

class NNGDBREPL:
    def __init__(self, debugger):
        self.debugger = debugger
        self.command_handler = CommandHandler(debugger)
        self.command_history = []
        self.setup_readline()

    def setup_readline(self):
        readline.parse_and_bind("tab: complete")
        readline.set_completer(rlcompleter.Completer(self.__dict__).complete)

    def run(self):
        print("Welcome to NNGDB (Neural Network GDB)")
        print("Type 'help' for a list of commands, or 'quit' to exit.")
        
        while True:
            try:
                user_input = input("<nngdb> ")
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                self.command_history.append(user_input)
                command_parts = user_input.split()
                if command_parts:
                    command, args = command_parts[0], command_parts[1:]
                    result = self.command_handler.handle_command(command, args)
                    print(result)
            except EOFError:
                break
            except Exception as e:
                print(f"Error: {str(e)}")
        
        print("Exiting NNGDB...")

    def get_command_completions(self, text, state):
        commands = self.command_handler.get_available_commands()
        matches = [cmd for cmd in commands if cmd.startswith(text)]
        return matches[state] if state < len(matches) else None
    
    def get_command_history(self):
        return self.command_history
    
    def complete(self, text, state):
        options = [cmd for cmd in self.command_handler.get_available_commands() if cmd.startswith(text)]
        if state < len(options):
            return options[state]
        else:
            return None
        
    def cmd_history(self, *args):
        """
        Show command history.
        Usage: history [n]
        """
        n = int(args[0]) if args else len(self.command_history)
        return "\n".join(self.command_history[-n:])

# cli/python_repl.py
import code
import readline
import rlcompleter

class PythonREPL:
    def __init__(self, debugger):
        self.debugger = debugger

    def run(self):
        print("Entering Python REPL. Use 'debugger' to access the NNGDB instance.")
        print("Type 'exit()' or press Ctrl+D to return to NNGDB.")

        # Set up readline with tab completion
        readline.parse_and_bind("tab: complete")
        
        # Create a dictionary of local variables for the interactive console
        local_vars = {'debugger': self.debugger}
        
        # Start the interactive console
        code.InteractiveConsole(local_vars).interact(banner="")

# cli/command_handler.py
from typing import List
from core.debugger import NNGDB
from advanced.interpretability_metrics import InterpretabilityMetrics
from advanced.explainability_techniques import ExplainabilityTechniques
from utils.error_handling import handle_exceptions
from transformers import AutoTokenizer
import torch

class CommandHandler:
    def __init__(self, debugger: NNGDB):
        self.debugger = debugger

    @handle_exceptions
    def cmd_run(self, *args):
        """
        Run the model with the given input.
        Usage: run <input_text>
        """
        if not args:
            return "Usage: run <input_text>"
        input_text = " ".join(args)
        return self.debugger.run(input_text)

    @handle_exceptions
    def handle_command(self, command: str, args: List[str]) -> str:
        method_name = f"cmd_{command}"
        if hasattr(self, method_name):
            method = getattr(self, method_name)
            return method(*args)
        else:
            return f"Unknown command: {command}"
    
    @handle_exceptions
    def get_available_commands(self) -> List[str]:
        return [method[4:] for method in dir(self) if method.startswith("cmd_")]
    
    @handle_exceptions
    def cmd_help(self, *args):
        if not args:
            commands = self.get_available_commands()
            return "Available commands:\n" + "\n".join(commands)
        else:
            method_name = f"cmd_{args[0]}"
            if hasattr(self, method_name):
                method = getattr(self, method_name)
                return method.__doc__ or "No help available for this command."
            else:
                return f"Unknown command: {args[0]}"

    @handle_exceptions
    def cmd_inspect(self, *args):
        """
        Inspect a layer, weight, activation, or gradient.
        Usage: inspect <type> <name>
        Types: model, layer, weight, activation, gradient
        """
        if len(args) < 1:
            return "Usage: inspect <type> [<name>]"
        inspect_type = args[0]
        if inspect_type == "model":
            return self.debugger.inspect_model()
        elif len(args) < 2:
            return "Usage: inspect <type> <name>"
        name = args[1]
        if inspect_type == "layer":
            return self.debugger.inspect_layer(name)
        elif inspect_type == "weight":
            return self.debugger.inspect_weights(name)
        elif inspect_type == "activation":
            return self.debugger.inspect_activations(name)
        elif inspect_type == "gradient":
            return self.debugger.inspect_gradients(name)
        else:
            return f"Unknown inspection type: {inspect_type}"
    
    @handle_exceptions
    def cmd_breakpoint(self, *args):
        """
        Set or remove a breakpoint.
        Usage: breakpoint set <layer_name> [<condition>]
               breakpoint remove <layer_name>
               breakpoint list
        """
        if not args:
            return "Usage: breakpoint <set|remove|list> [<layer_name>] [<condition>]"
        action = args[0]
        if action == "set":
            if len(args) < 2:
                return "Usage: breakpoint set <layer_name> [<condition>]"
            layer_name = args[1]
            condition = " ".join(args[2:]) if len(args) > 2 else None
            return self.debugger.set_breakpoint(layer_name, condition)
        elif action == "remove":
            if len(args) < 2:
                return "Usage: breakpoint remove <layer_name>"
            layer_name = args[1]
            return self.debugger.remove_breakpoint(layer_name)
        elif action == "list":
            return self.debugger.list_breakpoints()
        else:
            return f"Unknown breakpoint action: {action}"
    
    @handle_exceptions
    def cmd_step(self, *args):
        """
        Step through execution.
        Usage: step [<num_steps>]
        """
        num_steps = int(args[0]) if args else 1
        return self.debugger.step(num_steps)

    @handle_exceptions
    def cmd_continue(self, *args):
        """
        Continue execution after hitting a breakpoint.
        Usage: continue
        """
        return self.debugger.continue_execution()

    @handle_exceptions
    def cmd_modify(self, *args):
        """
        Modify weights or activations in the model.
        Usage: 
            modify weight <layer_name> <weight_name> <indices> <value>
            modify activation <layer_name> <function>
        Examples:
            modify weight model.layers.0.self_attn.q_proj weight (0,0) 1.0
            modify activation model.layers.0.self_attn "x * 2"
        """
        if len(args) < 3:
            return "Usage: modify <type> <layer_name> <details>"
        
        modify_type = args[0]
        layer_name = args[1]
        
        if modify_type == "weight":
            if len(args) != 5:
                return "Usage: modify weight <layer_name> <weight_name> <indices> <value>"
            weight_name = args[2]
            indices = eval(args[3])  # Be careful with eval, ensure proper input validation
            value = float(args[4])
            return self.debugger.modify_weight(layer_name, weight_name, indices, value)
        
        elif modify_type == "activation":
            function_str = " ".join(args[2:])
            return self.debugger.modify_activation(layer_name, function_str)
        
        else:
            return f"Unknown modification type: {modify_type}"
    
    @handle_exceptions
    def cmd_analyze(self, *args):
        """
        Perform analysis on the model.
        Usage: analyze <type> [<args>]
        Types: gradients, attention, activations
        """
        if not args:
            return "Usage: analyze <type> [<args>]"
        analysis_type = args[0]
        if analysis_type == "gradients":
            return self.debugger.analyze_gradients()
        elif analysis_type == "attention":
            return self.debugger.analyze_attention()
        elif analysis_type == "activations":
            return self.debugger.analyze_activations()
        else:
            return f"Unknown analysis type: {analysis_type}"
    
    @handle_exceptions
    def cmd_log(self, *args):
        """
        Log information or export logs.
        Usage: log <action> [<args>]
        Actions: info, warning, error, export
        """
        if not args:
            return "Usage: log <action> [<args>]"
        action = args[0]
        message = " ".join(args[1:])
        if action == "info":
            self.debugger.log_info(message)
            return "Info logged."
        elif action == "warning":
            self.debugger.log_warning(message)
            return "Warning logged."
        elif action == "error":
            self.debugger.log_error(message)
            return "Error logged."
        elif action == "export":
            return self.debugger.export_logs(message)
        else:
            return f"Unknown log action: {action}"

    @handle_exceptions
    def cmd_python(self, *args):
        """
        Enter Python REPL for custom analysis.
        Usage: python
        """
        return self.debugger.enter_python_repl()
    
    @handle_exceptions
    def cmd_trace(self, *args):
        """
        Start tracing execution, activations, or gradients.
        Usage: trace <type>
        Types: execution, activations, gradients
        """
        if not args:
            return "Usage: trace <type> (execution, activations, or gradients)"
        trace_type = args[0]
        if trace_type == "execution":
            return self.debugger.trace_execution()
        elif trace_type == "activations":
            return self.debugger.trace_activations()
        elif trace_type == "gradients":
            return self.debugger.trace_gradients()
        else:
            return f"Unknown trace type: {trace_type}"
    
    @handle_exceptions
    def cmd_get_trace(self, *args):
        """
        Get the trace for execution, activations, or gradients.
        Usage: get_trace <type> [<layer_name>]
        Types: execution, activations, gradients
        """
        if not args:
            return "Usage: get_trace <type> [<layer_name>]"
        trace_type = args[0]
        if trace_type == "execution":
            return self.debugger.get_execution_trace()
        elif trace_type == "activations":
            if len(args) < 2:
                return "Usage: get_trace activations <layer_name>"
            layer_name = args[1]
            return self.debugger.get_activation_trace(layer_name)
        elif trace_type == "gradients":
            if len(args) < 2:
                return "Usage: get_trace gradients <layer_name>"
            layer_name = args[1]
            return self.debugger.get_gradient_trace(layer_name)
        else:
            return f"Unknown trace type: {trace_type}"
    
    @handle_exceptions
    def cmd_clear_traces(self, *args):
        """
        Clear all traces.
        Usage: clear_traces
        """
        return self.debugger.clear_all_traces()
    
    @handle_exceptions
    def cmd_analyze_tokens(self, *args):
        """
        Analyze token probabilities for the given input.
        Usage: analyze_tokens <input_text> [top_k]
        """
        if not args:
            return "Usage: analyze_tokens <input_text> [top_k]"
        input_text = " ".join(args[:-1]) if len(args) > 1 and args[-1].isdigit() else " ".join(args)
        top_k = int(args[-1]) if len(args) > 1 and args[-1].isdigit() else 5
        return self.debugger.analyze_token_probabilities(input_text, top_k)
    
    @handle_exceptions
    def cmd_compare_tokens(self, *args):
        """
        Compare token probabilities between two analyses.
        Usage: compare_tokens <index1> <index2>
        """
        if len(args) != 2:
            return "Usage: compare_tokens <index1> <index2>"
        try:
            index1, index2 = map(int, args)
            return self.debugger.compare_token_probabilities(index1, index2)
        except ValueError:
            return "Invalid indices. Please provide two integer values."
    
    @handle_exceptions
    def cmd_undo(self, *args):
        """
        Undo the last action.
        Usage: undo
        """
        return self.debugger.undo()

    @handle_exceptions
    def cmd_redo(self, *args):
        """
        Redo the last undone action.
        Usage: redo
        """
        return self.debugger.redo()
    
    @handle_exceptions
    def cmd_token_attention(self, *args):
        """
        Get attention weights for a specific layer and attention head.
        Usage: token_attention <layer_name> <head_index>
        """
        if len(args) != 2:
            return "Usage: token_attention <layer_name> <head_index>"
        layer_name, head_index = args[0], int(args[1])
        return self.debugger.get_token_attention(layer_name, head_index)

    @handle_exceptions
    def cmd_token_representation(self, *args):
        """
        Get token representations for a specific layer.
        Usage: token_representation <layer_name>
        """
        if len(args) != 1:
            return "Usage: token_representation <layer_name>"
        layer_name = args[0]
        return self.debugger.get_token_representation(layer_name)
    
    @handle_exceptions
    def cmd_modify_weight(self, *args):
        """
        Modify a weight in the model.
        Usage: modify_weight <layer_name> <weight_name> <indices> <value>
        """
        if len(args) < 4:
            return "Usage: modify_weight <layer_name> <weight_name> <indices> <value>"
        layer_name = args[0]
        weight_name = args[1]
        indices = eval(args[2])  # Be careful with eval, ensure proper input validation
        value = float(args[3])
        return self.debugger.modify_weight(layer_name, weight_name, indices, value)
    
    @handle_exceptions
    def cmd_reset_weights(self, *args):
        """
        Reset all modified weights to their original values.
        Usage: reset_weights
        """
        return self.debugger.reset_modified_weights()

    @handle_exceptions
    def cmd_analyze_tokens_modified(self, *args):
        """
        Analyze token probabilities with modified weights and compare to original.
        Usage: analyze_tokens_modified <input_text> [top_k]
        """
        if not args:
            return "Usage: analyze_tokens_modified <input_text> [top_k]"
        input_text = " ".join(args[:-1]) if len(args) > 1 and args[-1].isdigit() else " ".join(args)
        top_k = int(args[-1]) if len(args) > 1 and args[-1].isdigit() else 5
        return self.debugger.analyze_tokens_with_modified_weights(input_text, top_k)
    
    @handle_exceptions
    def cmd_interpretability(self, *args):
        """
        Compute interpretability metrics.
        Usage: interpretability <metric_name> [<args>]
        """
        if not args:
            return "Usage: interpretability <metric_name> [<args>]"
        metric_name = args[0]
        metric_args = args[1:]
        
        metrics = InterpretabilityMetrics()
        if hasattr(metrics, metric_name):
            method = getattr(metrics, metric_name)
            return method(self.debugger.wrapped_model.model, *metric_args)
        else:
            return f"Unknown metric: {metric_name}"
    
    @handle_exceptions
    def cmd_explainability(self, *args):
        """
        Apply explainability techniques.
        Usage: explainability <technique_name> [<args>]
        """
        if not args:
            return "Usage: explainability <technique_name> [<args>]"
        technique_name = args[0]
        technique_args = args[1:]
        
        techniques = ExplainabilityTechniques()
        if hasattr(techniques, technique_name):
            method = getattr(techniques, technique_name)
            return method(self.debugger.wrapped_model.model, *technique_args)
        else:
            return f"Unknown technique: {technique_name}"
    
    @handle_exceptions
    def cmd_add_hook(self, *args):
        """
        Add a custom hook to a module.
        Usage: add_hook <forward|backward> <module_name> <hook_name> <hook_function>
        """
        if len(args) < 4:
            return "Usage: add_hook <forward|backward> <module_name> <hook_name> <hook_function>"
        
        hook_type = args[0]
        module_name = args[1]
        hook_name = args[2]
        hook_function = " ".join(args[3:])
        
        return self.debugger.add_hook(hook_type, module_name, hook_name, hook_function)

    @handle_exceptions
    def cmd_remove_hook(self, *args):
        """
        Remove a custom hook.
        Usage: remove_hook <hook_name>
        """
        if len(args) != 1:
            return "Usage: remove_hook <hook_name>"
        
        hook_name = args[0]
        return self.debugger.remove_hook(hook_name)
    
    @handle_exceptions
    def cmd_list_hooks(self, *args):
        """
        List all registered custom hooks.
        Usage: list_hooks
        """
        return self.debugger.list_hooks()
    
    @handle_exceptions
    def cmd_clear_hooks(self, *args):
        """
        Clear all custom hooks.
        Usage: clear_hooks
        """
        return self.debugger.clear_hooks()


# modification/__init__.py
from .weight_modifier import WeightModifier
from .activation_modifier import ActivationModifier
from .hyperparameter_modifier import HyperparameterModifier
from .model_surgery import ModelSurgeon

__all__ = ['WeightModifier', 'ActivationModifier', 'HyperparameterModifier', 'ModelSurgeon']

# modification/weight_modifier.py
import torch
from core.model_wrapper import ModelWrapper

class WeightModifier:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def modify_weight(self, layer_name: str, weight_name: str, indices, value):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        if not hasattr(layer, weight_name):
            return f"Weight '{weight_name}' not found in layer '{layer_name}'."

        weight = getattr(layer, weight_name)
        if not isinstance(weight, torch.Tensor):
            return f"'{weight_name}' is not a tensor in layer '{layer_name}'."

        try:
            with torch.no_grad():
                weight[indices] = value
            return f"Weight at {layer_name}.{weight_name}{indices} modified to {value}"
        except Exception as e:
            return f"Error modifying weight: {str(e)}"

    def scale_weights(self, layer_name: str, weight_name: str, scale_factor: float):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        if not hasattr(layer, weight_name):
            return f"Weight '{weight_name}' not found in layer '{layer_name}'."

        weight = getattr(layer, weight_name)
        if not isinstance(weight, torch.Tensor):
            return f"'{weight_name}' is not a tensor in layer '{layer_name}'."

        with torch.no_grad():
            weight.mul_(scale_factor)

        return f"Weights in {layer_name}.{weight_name} scaled by {scale_factor}"

    def reset_weights(self, layer_name: str):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        def weight_reset(m):
            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):
                m.reset_parameters()

        layer.apply(weight_reset)
        return f"Weights in layer '{layer_name}' have been reset."

    def add_noise_to_weights(self, layer_name: str, weight_name: str, noise_scale: float):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        if not hasattr(layer, weight_name):
            return f"Weight '{weight_name}' not found in layer '{layer_name}'."

        weight = getattr(layer, weight_name)
        if not isinstance(weight, torch.Tensor):
            return f"'{weight_name}' is not a tensor in layer '{layer_name}'."

        with torch.no_grad():
            noise = torch.randn_like(weight) * noise_scale
            weight.add_(noise)

        return f"Noise added to weights in {layer_name}.{weight_name} with scale {noise_scale}"

    def prune_weights(self, layer_name: str, weight_name: str, threshold: float):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        if not hasattr(layer, weight_name):
            return f"Weight '{weight_name}' not found in layer '{layer_name}'."

        weight = getattr(layer, weight_name)
        if not isinstance(weight, torch.Tensor):
            return f"'{weight_name}' is not a tensor in layer '{layer_name}'."

        with torch.no_grad():
            mask = (weight.abs() > threshold).float()
            weight.mul_(mask)

        pruned_percentage = (1 - mask.mean().item()) * 100
        return f"Pruned {pruned_percentage:.2f}% of weights in {layer_name}.{weight_name}"

# modification/hyperparameter_modifier.py
from core.model_wrapper import ModelWrapper
import torch

class HyperparameterModifier:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def modify_learning_rate(self, optimizer, new_lr: float):
        if not hasattr(self.wrapped_model, 'optimizer'):
            return "No optimizer found. Please set an optimizer for the model first."

        for param_group in optimizer.param_groups:
            param_group['lr'] = new_lr

        return f"Learning rate modified to {new_lr}"

    def modify_weight_decay(self, optimizer, new_weight_decay: float):
        if not hasattr(self.wrapped_model, 'optimizer'):
            return "No optimizer found. Please set an optimizer for the model first."

        for param_group in optimizer.param_groups:
            param_group['weight_decay'] = new_weight_decay

        return f"Weight decay modified to {new_weight_decay}"

    def modify_dropout_rate(self, dropout_rate: float):
        modified_layers = []
        for name, module in self.wrapped_model.model.named_modules():
            if isinstance(module, torch.nn.Dropout):
                module.p = dropout_rate
                modified_layers.append(name)

        if modified_layers:
            return f"Dropout rate modified to {dropout_rate} for layers: {', '.join(modified_layers)}"
        else:
            return "No dropout layers found in the model."

    def freeze_layers(self, layer_names):
        frozen_layers = []
        for name, param in self.wrapped_model.model.named_parameters():
            if any(layer_name in name for layer_name in layer_names):
                param.requires_grad = False
                frozen_layers.append(name)

        if frozen_layers:
            return f"Layers frozen: {', '.join(frozen_layers)}"
        else:
            return "No layers matched the provided names."

    def unfreeze_layers(self, layer_names):
        unfrozen_layers = []
        for name, param in self.wrapped_model.model.named_parameters():
            if any(layer_name in name for layer_name in layer_names):
                param.requires_grad = True
                unfrozen_layers.append(name)

        if unfrozen_layers:
            return f"Layers unfrozen: {', '.join(unfrozen_layers)}"
        else:
            return "No layers matched the provided names."

# modification/model_surgery.py
import torch
import torch.nn as nn
from core.model_wrapper import ModelWrapper

class ModelSurgeon:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model

    def replace_layer(self, layer_name: str, new_layer: nn.Module):
        parent_name, child_name = layer_name.rsplit('.', 1)
        parent_module = self.wrapped_model.get_layer(parent_name)

        if parent_module is None:
            return f"Parent module of '{layer_name}' not found."

        if not hasattr(parent_module, child_name):
            return f"Layer '{child_name}' not found in '{parent_name}'."

        setattr(parent_module, child_name, new_layer)
        return f"Layer '{layer_name}' replaced with {type(new_layer).__name__}"

    def insert_layer(self, layer_name: str, new_layer: nn.Module, position: str = 'after'):
        parent_name, child_name = layer_name.rsplit('.', 1)
        parent_module = self.wrapped_model.get_layer(parent_name)

        if parent_module is None:
            return f"Parent module of '{layer_name}' not found."

        if not hasattr(parent_module, child_name):
            return f"Layer '{child_name}' not found in '{parent_name}'."

        original_layer = getattr(parent_module, child_name)
        
        class WrappedLayer(nn.Module):
            def __init__(self, original_layer, new_layer, position):
                super().__init__()
                self.original_layer = original_layer
                self.new_layer = new_layer
                self.position = position

            def forward(self, x):
                if self.position == 'before':
                    x = self.new_layer(x)
                    return self.original_layer(x)
                elif self.position == 'after':
                    x = self.original_layer(x)
                    return self.new_layer(x)

        wrapped_layer = WrappedLayer(original_layer, new_layer, position)
        setattr(parent_module, child_name, wrapped_layer)

        return f"Layer '{new_layer.__class__.__name__}' inserted {position} '{layer_name}'"

    def remove_layer(self, layer_name: str):
        parent_name, child_name = layer_name.rsplit('.', 1)
        parent_module = self.wrapped_model.get_layer(parent_name)

        if parent_module is None:
            return f"Parent module of '{layer_name}' not found."

        if not hasattr(parent_module, child_name):
            return f"Layer '{child_name}' not found in '{parent_name}'."

        class Identity(nn.Module):
            def __init__(self):
                super().__init__()

            def forward(self, x):
                return x

        setattr(parent_module, child_name, Identity())
        return f"Layer '{layer_name}' removed and replaced with Identity"

    def change_activation_function(self, layer_name: str, new_activation: nn.Module):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        if not hasattr(layer, 'activation'):
            return f"Layer '{layer_name}' does not have an 'activation' attribute."

        layer.activation = new_activation
        return f"Activation function of '{layer_name}' changed to {type(new_activation).__name__}"

# modification/activation_modifier.py
import torch
from core.model_wrapper import ModelWrapper

class ActivationModifier:
    def __init__(self, wrapped_model: ModelWrapper):
        self.wrapped_model = wrapped_model
        self.hooks = {}

    def modify_activation(self, layer_name: str, modification_function):
        layer = self.wrapped_model.get_layer(layer_name)
        if layer is None:
            return f"Layer '{layer_name}' not found."

        def hook(module, input, output):
            return modification_function(output)

        handle = layer.register_forward_hook(hook)
        self.hooks[layer_name] = handle

        return f"Activation modification hook set for layer '{layer_name}'"

    def remove_modification(self, layer_name: str):
        if layer_name in self.hooks:
            self.hooks[layer_name].remove()
            del self.hooks[layer_name]
            return f"Activation modification removed for layer '{layer_name}'"
        else:
            return f"No activation modification found for layer '{layer_name}'"

    def clear_all_modifications(self):
        for handle in self.hooks.values():
            handle.remove()
        self.hooks.clear()
        return "All activation modifications cleared"

    def add_noise_to_activation(self, layer_name: str, noise_scale: float):
        def add_noise(output):
            return output + torch.randn_like(output) * noise_scale

        return self.modify_activation(layer_name, add_noise)

    def clip_activation(self, layer_name: str, min_val: float, max_val: float):
        def clip(output):
            return torch.clamp(output, min_val, max_val)

        return self.modify_activation(layer_name, clip)

    def scale_activation(self, layer_name: str, scale_factor: float):
        def scale(output):
            return output * scale_factor

        return self.modify_activation(layer_name, scale)

